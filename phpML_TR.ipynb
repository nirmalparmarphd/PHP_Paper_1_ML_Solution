{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "# ML Models\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression, ElasticNet\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import xgboost as xgb\n",
    "# tune\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials, STATUS_FAIL\n",
    "from mlflow.sklearn import log_model, save_model\n",
    "from mlflow.tracking import MlflowClient\n",
    "import mlflow.xgboost\n",
    "import mlflow.sklearn\n",
    "import mlflow\n",
    "from mlflow.models.signature import infer_signature\n",
    "from urllib.parse import urlparse\n",
    "import shap\n",
    "import logging\n",
    "import sys\n",
    "import warnings\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## data loading\n",
    "df = pd.read_csv('data/php_data_all.csv', index_col=0)\n",
    "# selecting data according to temperature range\n",
    "# NOTE: Data selected between [300, 355]\n",
    "df = df[(df['Te[K]'] > 300) & (df['Te[K]'] < 355)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TR Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## data split\n",
    "x = df[['Te[K]', 'P[bar]', 'Q[W]', 'Fluid', 'FR']]\n",
    "y = df['TR[K/W]']\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "## data pipeline preparation\n",
    "numeric_features = ['Te[K]', 'P[bar]', 'Q[W]','FR']\n",
    "categorical_features = ['Fluid']\n",
    "\n",
    "numeric_transformer = make_pipeline(StandardScaler())\n",
    "categorical_tranformer = make_pipeline(OneHotEncoder(sparse_output=False, handle_unknown='ignore'))\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "    ('num', numeric_transformer, numeric_features),\n",
    "    ('cat', categorical_tranformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "\n",
    "## a function to evaluate a trained ML model\n",
    "def evaluate(y_test, y_pred, k=6):\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    n = y_test.shape[0]\n",
    "    k = k\n",
    "    r2_adj = 1 - (((1-r2)*(n-1)) / (n-k-1))\n",
    "    return rmse, mae, r2, r2_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## objective of a ML model training\n",
    "# mlflow manual logging of metrics and model\n",
    "mlflow.set_experiment('CLPHP_TR_Prediction')\n",
    "## Model dictionary\n",
    "models = [\n",
    "    {\n",
    "        'name': 'XGBoost',\n",
    "        'model': xgb.XGBRegressor,\n",
    "        'search_space': {\n",
    "            'eta': hp.uniform('eta', 0.1, 1),\n",
    "            'max_depth': hp.randint('max_depth', 2, 5)\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'name': 'Random Forest',\n",
    "        'model': RandomForestRegressor,\n",
    "        'search_space': {\n",
    "            'n_estimators': hp.randint('n_estimators', 11, 101),\n",
    "            'max_depth': hp.randint('max_depth', 2, 20)\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'name': 'Linear Regression',\n",
    "        'model': LinearRegression,\n",
    "        'search_space': {}  # No hyperparameters for Linear Regression\n",
    "    },\n",
    "    # {\n",
    "    #     'name': 'Elastic Net',\n",
    "    #     'model': ElasticNet,\n",
    "    #     'search_space': {\n",
    "    #         'alpha': hp.uniform('alpha', 0.1, 1),\n",
    "    #         'l1_ratio': hp.uniform('l1_ratio', 0.1, 0.9)\n",
    "    #     }\n",
    "    # },\n",
    "    {\n",
    "        'name': 'Gradient Boosting',\n",
    "        'model': GradientBoostingRegressor,\n",
    "        'search_space': {\n",
    "            'n_estimators': hp.randint('n_estimators', 50, 150),\n",
    "            'learning_rate': hp.uniform('learning_rate', 0.01, 0.1),\n",
    "            'max_depth': hp.randint('max_depth', 2, 10)\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'name': 'Support Vector Machine',\n",
    "        'model': SVR,\n",
    "        'search_space': {\n",
    "            'C': hp.loguniform('C', -5, 2),\n",
    "            'kernel': hp.choice('kernel', ['linear', 'rbf', 'poly']),\n",
    "            'degree': hp.randint('degree', 2, 5)\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'name': 'K-Nearest Neighbors',\n",
    "        'model': KNeighborsRegressor,\n",
    "        'search_space': {\n",
    "            'n_neighbors': hp.randint('n_neighbors', 3, 20),\n",
    "            'weights': hp.choice('weights', ['uniform', 'distance']),\n",
    "            'p': hp.choice('p', [1, 2])\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "all_predictions = {}\n",
    "best_models = []\n",
    "# Loop through the list of models and train/tune each model\n",
    "for model_info in models:\n",
    "    model_name = model_info['name']\n",
    "    model_class = model_info['model']\n",
    "    search_space = model_info['search_space']\n",
    "\n",
    "    def objective(params):\n",
    "        try:\n",
    "            # Check if there is an active run and end it\n",
    "            if mlflow.active_run() is not None:\n",
    "                mlflow.end_run()\n",
    "                \n",
    "            with mlflow.start_run(run_name=f'{model_name}_run'):\n",
    "                mlflow.set_tag('model', model_name)\n",
    "                mlflow.log_params(params=params)\n",
    "\n",
    "                # data pipeline\n",
    "                if search_space:\n",
    "                    model = model_class(**params)\n",
    "                else:\n",
    "                    model = model_class()\n",
    "\n",
    "                data_pipeline = Pipeline(steps=[('Preprocessing', preprocessor), \n",
    "                                                (model_name, model)])\n",
    "                \n",
    "                # cross validation\n",
    "                cv_scores = cross_val_score(data_pipeline, x_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "                avg_cv_rmse = np.sqrt(-cv_scores.mean())\n",
    "                mlflow.log_metric('avg_cv_rmse', avg_cv_rmse)\n",
    "                \n",
    "                # training\n",
    "                data_pipeline.fit(x_train, y_train)\n",
    "\n",
    "                # train and test prediction\n",
    "                #pred_train = data_pipeline.predict(x_train)\n",
    "                pred = data_pipeline.predict(x_test)\n",
    "                rmse, mae, r2, r2_adj = evaluate(y_test=y_test, y_pred=pred)\n",
    "                signature = infer_signature(x_train, pred)\n",
    "\n",
    "                # collecting pred\n",
    "                all_predictions[model_name] = {'true_labels': y_test,\n",
    "                                               'predicted_labels': pred}\n",
    "                # test\n",
    "                mlflow.log_metric('rmse', rmse)\n",
    "                mlflow.log_metric('mae', mae)\n",
    "                mlflow.log_metric('r2', r2)\n",
    "                mlflow.log_metric('r2_adj', r2_adj)\n",
    "                \n",
    "                mlflow.sklearn.log_model(\n",
    "                    sk_model=model,\n",
    "                    artifact_path=f\"{model_name.lower()}-model\",\n",
    "                    signature=signature,\n",
    "                    registered_model_name=f\"{model_name.lower()}-regressor\",\n",
    "                )\n",
    "            \n",
    "            return  {'loss': rmse, 'status': STATUS_OK}\n",
    "        \n",
    "        except Exception as err:\n",
    "            # Log the error and continue to the next model\n",
    "            print(f\"Error occurred for {model_name}: {str(err)}\")\n",
    "            traceback.print_exc()  # Print detailed error traceback\n",
    "            return {'loss': float('inf'), 'status': STATUS_FAIL}  # Set a high loss value to minimize impact on optimization\n",
    "\n",
    "    try:\n",
    "        if search_space:\n",
    "            # Hyperparameter tuning\n",
    "            best_results = fmin(fn=objective, \n",
    "                                space=search_space, \n",
    "                                algo=tpe.suggest, \n",
    "                                max_evals=10, \n",
    "                                trials=Trials())\n",
    "            \n",
    "            # Get the best model based on the best_results or other criteria\n",
    "            best_model = model_class(**best_results)\n",
    "\n",
    "            # Log hyperparameters of the best model\n",
    "            mlflow.log_params(params=best_results)\n",
    "        else:\n",
    "            best_model = model_class()\n",
    "            \n",
    "        # Tag the best model with additional information\n",
    "        mlflow.set_tag('best_model_name', model_name)\n",
    "\n",
    "        best_models.append((model_name, best_model))\n",
    "        \n",
    "        # # Get the best model based on the best_results or other criteria\n",
    "        # best_model = model_class(**best_results)\n",
    "\n",
    "        # Save the best model (this may vary based on the model serialization method you are using)\n",
    "        save_model(best_model, f\"best_{model_name.lower()}_model\")\n",
    "\n",
    "        # Register the model in the Model Registry\n",
    "        model_uri = f\"runs:/{mlflow.active_run().info.run_id}/best_{model_name.lower()}_model\"\n",
    "        mlflow.register_model(model_uri, f\"{model_name.lower()}-regressor\")\n",
    "\n",
    "    except Exception as err:\n",
    "        # Log the error and continue to the next model\n",
    "        print(f\"Error occurred during hyperparameter tuning for {model_name}: {str(err)}\")\n",
    "        traceback.print_exc()  # Print detailed error traceback\n",
    "        continue  # Continue to the next iteration of the loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting predictions for comparison\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for model_name, predictions_data in all_predictions.items():\n",
    "    true_labels = predictions_data['true_labels']\n",
    "    predicted_labels = predictions_data['predicted_labels']\n",
    "    \n",
    "    plt.scatter(true_labels, predicted_labels, label=model_name, alpha=0.5)\n",
    "\n",
    "plt.xlabel('True Labels')\n",
    "plt.ylabel('Predicted Labels')\n",
    "plt.title('Model Predictions Comparison')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in best_models:\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
